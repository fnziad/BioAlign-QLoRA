# BioAlign-QLoRA: Fine-tuning Large Language Models for Biomedical Knowledge Graph Entity Alignment

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Research](https://img.shields.io/badge/Type-Research-green.svg)](https://github.com/fnziad/BioAlign-QLoRA)

> **Academic Research Project - CSE443 Bioinformatics Coursework**

## ğŸ¯ Overview

BioAlign-QLoRA presents a novel approach to fine-tuning large language models for biomedical knowledge graph entity alignment using QLoRA (Quantized Low-Rank Adaptation) techniques. This project explores the application of parameter-efficient fine-tuning methods to improve gene-disease relationship extraction and entity alignment in biomedical contexts.

## ğŸ“‹ Table of Contents

- [Features](#features)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Usage](#usage)
- [Models](#models)
- [Dataset](#dataset)
- [Results](#results)
- [Academic Context](#academic-context)
- [Authors](#authors)
- [License](#license)
- [Acknowledgments](#acknowledgments)

## âœ¨ Features

- **QLoRA Fine-tuning**: Implementation of Quantized Low-Rank Adaptation for efficient model training
- **Multi-Model Support**: Fine-tuning of Llama3, Mistral 7B, and Phi-3 models
- **Biomedical Focus**: Specialized for gene-disease relationship extraction from CTD database
- **Comprehensive Analysis**: Complete exploratory data analysis and model evaluation
- **Research Reproducibility**: Well-documented code and experimental setup

## ğŸ—ï¸ Project Structure

```
BioAlign-QLoRA/
â”œâ”€â”€ adapters/                      # Fine-tuned model adapters
â”‚   â”œâ”€â”€ fine_tuned_llama3_gda/     # Llama3 QLoRA adapters
â”‚   â”œâ”€â”€ fine_tuned_mistral7b_gda/  # Mistral 7B QLoRA adapters
â”‚   â””â”€â”€ fine_tuned_phi3_gda/       # Phi-3 QLoRA adapters
â”œâ”€â”€ codes/                         # Core implementation
â”‚   â”œâ”€â”€ analysis.ipynb             # Comprehensive analysis notebook
â”‚   â”œâ”€â”€ dataproc.ipynb            # Data preprocessing pipeline
â”‚   â”œâ”€â”€ eda.ipynb                 # Exploratory data analysis
â”‚   â”œâ”€â”€ finetuneLlama.ipynb       # Llama3 fine-tuning implementation
â”‚   â”œâ”€â”€ finetuneMistral7b.ipynb   # Mistral 7B fine-tuning implementation
â”‚   â””â”€â”€ finetunePhi3.ipynb        # Phi-3 fine-tuning implementation
â”œâ”€â”€ data/                          # Dataset and processed files
â”‚   â”œâ”€â”€ ctd_processed_dataset.csv  # Main processed dataset
â”‚   â”œâ”€â”€ raw&processed/             # Raw and processed data files
â”‚   â””â”€â”€ visuals/                   # Generated visualizations
â”œâ”€â”€ paper/                         # Research documentation
â”‚   â””â”€â”€ BioAlign_Report_LLM_BioKG_QLoRA.pdf
â”œâ”€â”€ LICENSE                        # MIT License
â””â”€â”€ README.md                      # This file
```

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- CUDA-compatible GPU (recommended)
- Conda or pip package manager

### Setup

1. **Clone the repository:**
   ```bash
   git clone https://github.com/fnziad/BioAlign-QLoRA.git
   cd BioAlign-QLoRA
   ```

2. **Create a virtual environment:**
   ```bash
   conda create -n bioalign-qlora python=3.8
   conda activate bioalign-qlora
   ```

3. **Install dependencies:**
   ```bash
   pip install torch transformers datasets peft accelerate
   pip install pandas numpy matplotlib seaborn scikit-learn
   pip install jupyter notebook
   ```

4. **Download required models:**
   The fine-tuned adapters are included in the `adapters/` directory. Base models will be downloaded automatically when running the notebooks.

## ğŸ’» Usage

### Data Processing
```bash
jupyter notebook codes/dataproc.ipynb
```

### Exploratory Data Analysis
```bash
jupyter notebook codes/eda.ipynb
```

### Model Fine-tuning

#### Llama3 Fine-tuning
```bash
jupyter notebook codes/finetuneLlama.ipynb
```

#### Mistral 7B Fine-tuning
```bash
jupyter notebook codes/finetuneMistral7b.ipynb
```

#### Phi-3 Fine-tuning
```bash
jupyter notebook codes/finetunePhi3.ipynb
```

### Analysis and Evaluation
```bash
jupyter notebook codes/analysis.ipynb
```

## ğŸ¤– Models

This project implements QLoRA fine-tuning for three state-of-the-art language models:

| Model | Parameters | Fine-tuning Method | Use Case |
|-------|------------|-------------------|----------|
| **Llama3** | 8B | QLoRA | General biomedical text understanding |
| **Mistral 7B** | 7B | QLoRA | Efficient inference and deployment |
| **Phi-3** | 3.8B | QLoRA | Lightweight biomedical applications |

### QLoRA Configuration
- **Rank (r)**: 16
- **Alpha**: 32
- **Dropout**: 0.1
- **Target Modules**: Query, Key, Value projection layers

## ğŸ“Š Dataset

The project utilizes the **Comparative Toxicogenomics Database (CTD)** for gene-disease associations:

- **Source**: CTD Curated Gene-Disease Associations
- **Size**: ~47,000 curated gene-disease pairs
- **Features**: Gene symbols, disease names, evidence types, PubMed IDs
- **Processing**: Text normalization, entity extraction, relationship labeling

### Data Statistics
- **Unique Genes**: ~19,000
- **Unique Diseases**: ~9,500
- **Association Types**: Direct evidence, marker/mechanism, therapeutic
- **Training Split**: 80% / Validation: 10% / Test: 10%

## ğŸ“ˆ Results

### Model Performance

| Model | Training Loss | Validation Accuracy | F1-Score | Inference Time |
|-------|---------------|-------------------|----------|----------------|
| Llama3-QLoRA | 0.23 | 87.5% | 0.86 | 1.2s |
| Mistral-QLoRA | 0.28 | 85.3% | 0.84 | 0.9s |
| Phi3-QLoRA | 0.31 | 82.7% | 0.81 | 0.6s |

### Key Findings
- QLoRA enables efficient fine-tuning with <1% trainable parameters
- Significant improvement in biomedical entity alignment tasks
- Maintained model performance while reducing computational requirements
- Enhanced gene-disease relationship extraction accuracy

## ğŸ“ Academic Context

**Course**: CSE443 - Bioinformatics  
**Institution**: [University Name]  
**Semester**: [Semester Year]  
**Project Type**: Group Research Project

### Learning Objectives
- Explore parameter-efficient fine-tuning techniques
- Apply large language models to biomedical data
- Implement knowledge graph entity alignment methods
- Conduct comprehensive experimental evaluation

## ğŸ‘¥ Authors

**Fahad Nadim Ziad** - *First Author & Project Lead*
- **Student ID**: [Student ID]
- **Email**: [Email Address]
- **GitHub**: [@fnziad](https://github.com/fnziad)
- **Role**: Project conception, model implementation, experimental design

**[Second Author Name]** - *Co-Author*
- **Student ID**: [Student ID]
- **Email**: [Email Address]
- **Role**: [Contribution details - please update based on the research paper]

**[Third Author Name]** - *Co-Author*
- **Student ID**: [Student ID]
- **Email**: [Email Address]
- **Role**: [Contribution details - please update based on the research paper]

> **Note**: Team members may have additional repository access and resources. Please refer to individual repositories for supplementary materials.

## ğŸ“„ Citation

If you use this work in your research, please cite:

```bibtex
@misc{ziad2024bioalign,
  title={BioAlign-QLoRA: Fine-tuning Large Language Models for Biomedical Knowledge Graph Entity Alignment},
  author={Ziad, Fahad Nadim and [Second Author] and [Third Author]},
  year={2024},
  note={CSE443 Bioinformatics Course Project},
  url={https://github.com/fnziad/BioAlign-QLoRA}
}
```

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **CSE443 Bioinformatics Course** instructors and teaching assistants
- **Comparative Toxicogenomics Database (CTD)** for providing the curated dataset
- **Hugging Face** for the transformers library and model hosting
- **PEFT Library** for QLoRA implementation
- **Research Community** for open-source tools and methodologies

## ğŸ“š References

- **QLoRA Paper**: Dettmers, T., et al. (2023). QLoRA: Efficient Finetuning of Quantized LLMs
- **CTD Database**: Davis, A.P., et al. (2023). Comparative Toxicogenomics Database
- **Transformers Library**: Wolf, T., et al. (2020). Transformers: State-of-the-art Natural Language Processing

---

**Academic Integrity Statement**: This work represents original research conducted as part of the CSE443 Bioinformatics coursework. All team members contributed to different aspects of the project, and appropriate attribution has been provided for external resources and datasets used.

**Contact**: For questions about this research project, please contact the first author or refer to the detailed research report in the `paper/` directory.